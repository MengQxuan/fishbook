# Fishbook - ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹

<div align="center">

[![Python Version](https://img.shields.io/badge/python-3.x-blue.svg)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/numpy-required-green.svg)](https://numpy.org/)
[![License](https://img.shields.io/badge/license-Educational-orange.svg)](LICENSE)

*ä»é›¶å¼€å§‹å­¦ä¹ æ·±åº¦å­¦ä¹  - ä½¿ç”¨ Python å’Œ NumPy æ„å»ºç¥ç»ç½‘ç»œ*

[English](#english-version) | [ä¸­æ–‡](#chinese-version)

</div>

---

## <a name="chinese-version"></a>ğŸ“š ä¸­æ–‡ç‰ˆæœ¬

### é¡¹ç›®ç®€ä»‹

æœ¬ä»“åº“æ˜¯ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹ä¸€ä¹¦çš„é…å¥—ä»£ç å®ç°ã€‚é€šè¿‡çº¯ Python å’Œ NumPyï¼Œä»é›¶å¼€å§‹æ„å»ºç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œå¸®åŠ©è¯»è€…æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒåŸç†ã€‚

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- ğŸ”§ **ä»é›¶å®ç°**ï¼šä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨ NumPy æ‰‹å·¥å®ç°æ‰€æœ‰ç»„ä»¶
- ğŸ“– **å¾ªåºæ¸è¿›**ï¼šæŒ‰ç« èŠ‚ç»„ç»‡ï¼Œä»æ„ŸçŸ¥æœºåˆ°å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€æ­¥æ·±å…¥
- ğŸ¯ **å®æˆ˜å¯¼å‘**ï¼šæ¯ç« åŒ…å«å¯è¿è¡Œçš„ç¤ºä¾‹ä»£ç å’Œ MNIST æ•°æ®é›†å®è·µ
- ğŸ§® **æ•°å­¦ç›´è§‚**ï¼šå°†æŠ½è±¡çš„æ•°å­¦æ¦‚å¿µè½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ Python ä»£ç 

### ç¯å¢ƒè¦æ±‚

**å¿…éœ€ä¾èµ–**ï¼š
- Python 3.x (æ¨è 3.6 æˆ–æ›´é«˜ç‰ˆæœ¬)
- NumPy >= 1.15.0
- Matplotlib >= 2.0.0
- Pillow (PIL) >= 5.0.0

### å¿«é€Ÿå¼€å§‹

#### 1. å…‹éš†ä»“åº“

```bash
git clone https://github.com/MengQxuan/fishbook.git
cd fishbook
```

#### 2. å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨ pip å®‰è£…
pip install numpy matplotlib pillow

# æˆ–ä½¿ç”¨ pip ä¸€æ¬¡æ€§å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt  # å¦‚æœæœ‰ requirements.txt
```

#### 3. ä¸‹è½½ MNIST æ•°æ®é›†

é¦–æ¬¡è¿è¡Œæ—¶ï¼ŒMNIST æ•°æ®é›†å°†è‡ªåŠ¨ä»é•œåƒæºä¸‹è½½ã€‚å¦‚éœ€æ‰‹åŠ¨ä¸‹è½½ï¼š

```bash
cd dataset
python mnist.py
```

#### 4. è¿è¡Œç¤ºä¾‹

```bash
# æµ‹è¯•æ„ŸçŸ¥æœº
python chapter2/and_gate.py

# è¿è¡Œ MNIST ç¥ç»ç½‘ç»œæ¨ç†
python chapter3/neuralnet_mnist.py

# è®­ç»ƒç¥ç»ç½‘ç»œ
python chapter4/train_neuralnet.py
```

### ğŸ“‚ é¡¹ç›®ç»“æ„è¯¦è§£

```plaintext
fishbook/
â”œâ”€â”€ chapter1/                   # ç¬¬1ç« ï¼šPython å…¥é—¨
â”‚   â””â”€â”€ hello.py               # Python åŸºç¡€ï¼šå˜é‡ã€ç±»å‹ã€ç±»ã€NumPy å¹¿æ’­
â”‚
â”œâ”€â”€ chapter2/                   # ç¬¬2ç« ï¼šæ„ŸçŸ¥æœº
â”‚   â”œâ”€â”€ and_gate.py            # AND é€»è¾‘é—¨å®ç°
â”‚   â”œâ”€â”€ or_gate.py             # OR é€»è¾‘é—¨å®ç°
â”‚   â”œâ”€â”€ nand_gate.py           # NAND é€»è¾‘é—¨å®ç°
â”‚   â””â”€â”€ xor_gate.py            # XOR é€»è¾‘é—¨ï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰
â”‚
â”œâ”€â”€ chapter3/                   # ç¬¬3ç« ï¼šç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ sigmoid.py             # Sigmoid æ¿€æ´»å‡½æ•°åŠå¯è§†åŒ–
â”‚   â”œâ”€â”€ relu.py                # ReLU æ¿€æ´»å‡½æ•°
â”‚   â”œâ”€â”€ step_function.py       # é˜¶è·ƒå‡½æ•°
â”‚   â”œâ”€â”€ forward.py             # å‰å‘ä¼ æ’­ç¤ºä¾‹
â”‚   â”œâ”€â”€ mnist_show.py          # æ˜¾ç¤º MNIST å›¾åƒ
â”‚   â”œâ”€â”€ neuralnet_mnist.py     # ç¥ç»ç½‘ç»œæ¨ç†ï¼ˆå•æ ·æœ¬ï¼‰
â”‚   â”œâ”€â”€ neuralnet_mnist_batch.py  # æ‰¹é‡æ¨ç†ï¼ˆæå‡æ€§èƒ½ï¼‰
â”‚   â””â”€â”€ sample_weight.pkl      # é¢„è®­ç»ƒæƒé‡æ–‡ä»¶
â”‚
â”œâ”€â”€ chapter4/                   # ç¬¬4ç« ï¼šç¥ç»ç½‘ç»œçš„å­¦ä¹ 
â”‚   â”œâ”€â”€ gradient_1d.py         # ä¸€ç»´å‡½æ•°çš„æ¢¯åº¦ï¼ˆæ•°å€¼å¾®åˆ†ï¼‰
â”‚   â”œâ”€â”€ gradient_2d.py         # äºŒç»´å‡½æ•°çš„æ¢¯åº¦å¯è§†åŒ–
â”‚   â”œâ”€â”€ gradient_method.py     # æ¢¯åº¦ä¸‹é™æ³•
â”‚   â”œâ”€â”€ gradient_simplenet.py  # ç®€å•ç½‘ç»œçš„æ¢¯åº¦è®¡ç®—
â”‚   â”œâ”€â”€ two_layer_net.py       # ä¸¤å±‚ç¥ç»ç½‘ç»œç±»
â”‚   â””â”€â”€ train_neuralnet.py     # å®Œæ•´è®­ç»ƒæµç¨‹
â”‚
â”œâ”€â”€ chapter5/                   # ç¬¬5ç« ï¼šè¯¯å·®åå‘ä¼ æ’­æ³•
â”‚   â”œâ”€â”€ buy_apple.py           # è®¡ç®—å›¾ç¤ºä¾‹ï¼ˆç®€å•ï¼‰
â”‚   â”œâ”€â”€ buy_apple_orange.py    # è®¡ç®—å›¾ç¤ºä¾‹ï¼ˆå¤æ‚ï¼‰
â”‚   â”œâ”€â”€ layer_naive.py         # ä¹˜æ³•å±‚å’ŒåŠ æ³•å±‚çš„ç®€å•å®ç°
â”‚   â”œâ”€â”€ gradient_check.py      # æ¢¯åº¦æ£€éªŒï¼ˆå¯¹æ¯”æ•°å€¼æ¢¯åº¦å’Œåå‘ä¼ æ’­ï¼‰
â”‚   â”œâ”€â”€ two_layer_net.py       # ä½¿ç”¨å±‚çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ
â”‚   â””â”€â”€ train_neuralnet.py     # ä½¿ç”¨åå‘ä¼ æ’­çš„è®­ç»ƒ
â”‚
â”œâ”€â”€ chapter6/                   # ç¬¬6ç« ï¼šä¸å­¦ä¹ ç›¸å…³çš„æŠ€å·§
â”‚   â”œâ”€â”€ optimizer_compare_naive.py    # ä¼˜åŒ–å™¨æ¯”è¾ƒï¼ˆç®€å•å‡½æ•°ï¼‰
â”‚   â”œâ”€â”€ optimizer_compare_mnist.py    # ä¼˜åŒ–å™¨åœ¨ MNIST ä¸Šçš„æ¯”è¾ƒ
â”‚   â”œâ”€â”€ weight_init_activation_histogram.py  # æƒé‡åˆå§‹åŒ–å¯¹æ¿€æ´»å€¼åˆ†å¸ƒçš„å½±å“
â”‚   â”œâ”€â”€ weight_init_compare.py        # æƒé‡åˆå§‹åŒ–æ–¹æ³•æ¯”è¾ƒ
â”‚   â”œâ”€â”€ batch_norm_test.py            # Batch Normalization æ•ˆæœæµ‹è¯•
â”‚   â”œâ”€â”€ overfit_weight_decay.py       # æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰
â”‚   â”œâ”€â”€ overfit_dropout.py            # Dropout æ­£åˆ™åŒ–
â”‚   â””â”€â”€ hyperparameter_optimization.py  # è¶…å‚æ•°ä¼˜åŒ–
â”‚
â”œâ”€â”€ chapter7/                   # ç¬¬7ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ simple_convnet.py      # ç®€å•å·ç§¯ç¥ç»ç½‘ç»œå®ç°
â”‚   â”œâ”€â”€ train_convnet.py       # è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ visualize_filter.py    # å¯è§†åŒ–å·ç§¯æ»¤æ³¢å™¨
â”‚   â””â”€â”€ params.pkl             # é¢„è®­ç»ƒçš„å·ç§¯ç½‘ç»œæƒé‡
â”‚
â”œâ”€â”€ chapter8/                   # ç¬¬8ç« ï¼šæ·±åº¦å­¦ä¹ 
â”‚   â”œâ”€â”€ deep_convnet.py        # æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ99%+ å‡†ç¡®ç‡ï¼‰
â”‚   â”œâ”€â”€ train_deepnet.py       # è®­ç»ƒæ·±åº¦ç½‘ç»œ
â”‚   â””â”€â”€ deep_convnet_params.pkl  # æ·±åº¦ç½‘ç»œé¢„è®­ç»ƒæƒé‡
â”‚
â”œâ”€â”€ common/                     # å…¬å…±æ¨¡å—
â”‚   â”œâ”€â”€ functions.py           # æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ gradient.py            # æ•°å€¼æ¢¯åº¦è®¡ç®—
â”‚   â”œâ”€â”€ layers.py              # å±‚çš„å®ç°ï¼ˆAffineã€Softmaxã€Convã€Poolingï¼‰
â”‚   â”œâ”€â”€ multi_layer_net.py     # å¤šå±‚ç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ multi_layer_net_extend.py  # æ‰©å±•çš„å¤šå±‚ç½‘ç»œï¼ˆæ”¯æŒ Dropoutã€BNï¼‰
â”‚   â”œâ”€â”€ optimizer.py           # ä¼˜åŒ–å™¨ï¼ˆSGDã€Momentumã€AdaGradã€Adamï¼‰
â”‚   â”œâ”€â”€ trainer.py             # è®­ç»ƒå™¨ï¼ˆå°è£…è®­ç»ƒå¾ªç¯ï¼‰
â”‚   â””â”€â”€ util.py                # è¾…åŠ©å‡½æ•°
â”‚
â””â”€â”€ dataset/                    # æ•°æ®é›†æ¨¡å—
    â”œâ”€â”€ mnist.py               # MNIST æ•°æ®åŠ è½½å™¨
    â””â”€â”€ mnist.pkl              # MNIST æ•°æ®é›†ï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½ï¼‰
```

### ğŸ“– ç« èŠ‚è¯¦ç»†è¯´æ˜

#### Chapter 1: Python å…¥é—¨
ä»‹ç» Python ç¼–ç¨‹åŸºç¡€ï¼ŒåŒ…æ‹¬å˜é‡ã€æ•°æ®ç±»å‹ã€ç±»ã€NumPy çš„åŸºæœ¬ä½¿ç”¨å’Œå¹¿æ’­æœºåˆ¶ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- Python åŸºæœ¬è¯­æ³•
- ç±»ä¸å¯¹è±¡
- NumPy æ•°ç»„æ“ä½œå’Œå¹¿æ’­

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter1/hello.py
```

---

#### Chapter 2: æ„ŸçŸ¥æœº
å®ç°æœ€ç®€å•çš„ç¥ç»ç½‘ç»œâ€”â€”æ„ŸçŸ¥æœºï¼Œå¹¶é€šè¿‡ç»„åˆå®ç°é€»è¾‘é—¨ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æ„ŸçŸ¥æœºçš„åŸºæœ¬åŸç†
- å®ç° ANDã€ORã€NAND é—¨
- å¤šå±‚æ„ŸçŸ¥æœºå®ç° XOR é—¨

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter2/and_gate.py     # è¾“å‡º: AND é—¨çœŸå€¼è¡¨
python chapter2/xor_gate.py     # å¤šå±‚æ„ŸçŸ¥æœºå®ç° XOR
```

**å­¦ä¹ è¦ç‚¹**ï¼š
- ç†è§£çº¿æ€§åˆ†ç±»å™¨çš„å±€é™æ€§
- äº†è§£å¤šå±‚æ„ŸçŸ¥æœºçš„è¡¨è¾¾èƒ½åŠ›

---

#### Chapter 3: ç¥ç»ç½‘ç»œ
æ·±å…¥ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ¦‚å¿µï¼ŒåŒ…æ‹¬æ¿€æ´»å‡½æ•°ã€å‰å‘ä¼ æ’­å’Œ MNIST æ•°æ®é›†çš„æ¨ç†ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æ¿€æ´»å‡½æ•°ï¼ˆSigmoidã€ReLUã€Step Functionï¼‰
- ä¸‰å±‚ç¥ç»ç½‘ç»œçš„å®ç°
- MNIST æ‰‹å†™æ•°å­—è¯†åˆ«
- æ‰¹å¤„ç†ä¼˜åŒ–

**å…³é”®æ–‡ä»¶**ï¼š
- `sigmoid.py` / `relu.py`: æ¿€æ´»å‡½æ•°çš„å®ç°å’Œå¯è§†åŒ–
- `neuralnet_mnist.py`: ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œæ¨ç†
- `neuralnet_mnist_batch.py`: æ‰¹é‡å¤„ç†æå‡æ¨ç†é€Ÿåº¦

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter3/mnist_show.py          # æ˜¾ç¤º MNIST å›¾åƒ
python chapter3/neuralnet_mnist.py     # ç¥ç»ç½‘ç»œæ¨ç†
python chapter3/neuralnet_mnist_batch.py  # æ‰¹é‡æ¨ç†
```

**é¢„æœŸç»“æœ**ï¼š
- è¯†åˆ«å‡†ç¡®ç‡çº¦ 93.52%

---

#### Chapter 4: ç¥ç»ç½‘ç»œçš„å­¦ä¹ 
å­¦ä¹ å¦‚ä½•é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è®­ç»ƒç¥ç»ç½‘ç»œã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æŸå¤±å‡½æ•°ï¼ˆå‡æ–¹è¯¯å·®ã€äº¤å‰ç†µï¼‰
- æ•°å€¼å¾®åˆ†å’Œæ¢¯åº¦è®¡ç®—
- æ¢¯åº¦ä¸‹é™æ³•
- mini-batch å­¦ä¹ 

**å…³é”®æ–‡ä»¶**ï¼š
- `gradient_1d.py` / `gradient_2d.py`: æ¢¯åº¦å¯è§†åŒ–
- `gradient_method.py`: æ¢¯åº¦ä¸‹é™æ³•æ¼”ç¤º
- `two_layer_net.py`: ä¸¤å±‚ç¥ç»ç½‘ç»œç±»
- `train_neuralnet.py`: å®Œæ•´çš„è®­ç»ƒæµç¨‹

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter4/gradient_2d.py      # å¯è§†åŒ–äºŒç»´æ¢¯åº¦
python chapter4/train_neuralnet.py  # è®­ç»ƒç¥ç»ç½‘ç»œ
```

**è®­ç»ƒè¾“å‡º**ï¼š
- é€ä¸ª epoch æ˜¾ç¤ºè®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡
- æŸå¤±å‡½æ•°å€¼çš„å˜åŒ–æ›²çº¿

---

#### Chapter 5: è¯¯å·®åå‘ä¼ æ’­æ³•
ä½¿ç”¨è®¡ç®—å›¾å’Œé“¾å¼æ³•åˆ™å®ç°é«˜æ•ˆçš„åå‘ä¼ æ’­ç®—æ³•ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- è®¡ç®—å›¾çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
- å„å±‚çš„åå‘ä¼ æ’­å®ç°ï¼ˆAffineã€ReLUã€Softmaxï¼‰
- æ¢¯åº¦æ£€éªŒ
- ä½¿ç”¨å±‚æ„å»ºç¥ç»ç½‘ç»œ

**å…³é”®æ–‡ä»¶**ï¼š
- `buy_apple.py`: è®¡ç®—å›¾åŸºç¡€ç¤ºä¾‹
- `layer_naive.py`: å±‚çš„ç®€å•å®ç°
- `gradient_check.py`: éªŒè¯åå‘ä¼ æ’­çš„æ­£ç¡®æ€§
- `train_neuralnet.py`: ä½¿ç”¨åå‘ä¼ æ’­è®­ç»ƒ

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter5/buy_apple_orange.py   # è®¡ç®—å›¾ç¤ºä¾‹
python chapter5/gradient_check.py     # æ¢¯åº¦æ£€éªŒ
python chapter5/train_neuralnet.py    # é«˜æ•ˆè®­ç»ƒ
```

**å­¦ä¹ è¦ç‚¹**ï¼š
- ç†è§£è®¡ç®—å›¾å’Œé“¾å¼æ³•åˆ™
- æŒæ¡åå‘ä¼ æ’­çš„é«˜æ•ˆæ€§ï¼ˆæ¯”æ•°å€¼å¾®åˆ†å¿« 100+ å€ï¼‰

---

#### Chapter 6: ä¸å­¦ä¹ ç›¸å…³çš„æŠ€å·§
æ·±åº¦å­¦ä¹ ä¸­çš„é«˜çº§æŠ€å·§ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ã€æƒé‡åˆå§‹åŒ–ã€æ­£åˆ™åŒ–å’Œè¶…å‚æ•°è°ƒä¼˜ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- **ä¼˜åŒ–å™¨**ï¼šSGDã€Momentumã€AdaGradã€RMSpropã€Adam
- **æƒé‡åˆå§‹åŒ–**ï¼šXavier åˆå§‹åŒ–ã€He åˆå§‹åŒ–
- **Batch Normalization**ï¼šåŠ é€Ÿè®­ç»ƒã€æé«˜æ³›åŒ–
- **æ­£åˆ™åŒ–**ï¼šWeight Decayã€Dropout
- **è¶…å‚æ•°ä¼˜åŒ–**ï¼šéªŒè¯é›†ã€ç½‘æ ¼æœç´¢

**å…³é”®æ–‡ä»¶**ï¼š
- `optimizer_compare_mnist.py`: å¯¹æ¯”å„ä¼˜åŒ–å™¨åœ¨ MNIST ä¸Šçš„è¡¨ç°
- `weight_init_compare.py`: æƒé‡åˆå§‹åŒ–æ–¹æ³•å¯¹æ¯”
- `batch_norm_test.py`: BN çš„æ•ˆæœ
- `overfit_dropout.py`: Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
- `hyperparameter_optimization.py`: è¶…å‚æ•°æœç´¢

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter6/optimizer_compare_mnist.py    # ä¼˜åŒ–å™¨å¯¹æ¯”
python chapter6/batch_norm_test.py            # BN æ•ˆæœæµ‹è¯•
python chapter6/overfit_dropout.py            # Dropout æ¼”ç¤º
```

**å­¦ä¹ è¦ç‚¹**ï¼š
- Adam é€šå¸¸æ˜¯æœ€ä½³é€‰æ‹©
- åˆç†çš„æƒé‡åˆå§‹åŒ–è‡³å…³é‡è¦
- Dropout å’Œ BN å¯æœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆ

---

#### Chapter 7: å·ç§¯ç¥ç»ç½‘ç»œ (CNN)
å®ç°ç”¨äºå›¾åƒè¯†åˆ«çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚

**ä¸»è¦å†…å®¹**ï¼š
- å·ç§¯å±‚ï¼ˆConvolutionï¼‰çš„å®ç°
- æ± åŒ–å±‚ï¼ˆPoolingï¼‰çš„å®ç°
- CNN çš„æ•´ä½“ç»“æ„
- å·ç§¯æ»¤æ³¢å™¨çš„å¯è§†åŒ–

**ç½‘ç»œç»“æ„**ï¼š
```
Input â†’ Conv â†’ ReLU â†’ Pooling â†’ Affine â†’ ReLU â†’ Affine â†’ Softmax
```

**å…³é”®æ–‡ä»¶**ï¼š
- `simple_convnet.py`: ç®€å• CNN å®ç°
- `train_convnet.py`: è®­ç»ƒ CNNï¼ˆå‡†ç¡®ç‡ ~98%ï¼‰
- `visualize_filter.py`: å¯è§†åŒ–å­¦åˆ°çš„æ»¤æ³¢å™¨

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter7/train_convnet.py       # è®­ç»ƒ CNN
python chapter7/visualize_filter.py    # å¯è§†åŒ–æ»¤æ³¢å™¨
```

---

#### Chapter 8: æ·±åº¦å­¦ä¹ 
æ„å»ºæ›´æ·±çš„ç½‘ç»œï¼Œæ¢ç´¢æ·±åº¦å­¦ä¹ çš„å¨åŠ›ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ
- ç½‘ç»œåŠ æ·±çš„æŠ€å·§
- æ•°æ®å¢å¼º
- è¿ç§»å­¦ä¹ çš„åŸºç¡€

**ç½‘ç»œç»“æ„**ï¼š
```
Conv-ReLU-Conv-ReLU-Pool Ã—3 â†’ Affine-ReLU-Dropout-Affine-Dropout-Softmax
```

**å…³é”®æ–‡ä»¶**ï¼š
- `deep_convnet.py`: æ·±åº¦ CNNï¼ˆ99%+ å‡†ç¡®ç‡ï¼‰
- `train_deepnet.py`: è®­ç»ƒæ·±åº¦ç½‘ç»œ

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
python chapter8/train_deepnet.py       # è®­ç»ƒæ·±åº¦ç½‘ç»œï¼ˆè€—æ—¶è¾ƒé•¿ï¼‰
```

**é¢„æœŸç»“æœ**ï¼š
- MNIST æµ‹è¯•é›†å‡†ç¡®ç‡ > 99%

---

### ğŸ§° å…¬å…±æ¨¡å—è¯¦è§£

#### `common/functions.py`
æ ¸å¿ƒæ•°å­¦å‡½æ•°å®ç°ã€‚

**æ¿€æ´»å‡½æ•°**ï¼š
- `sigmoid(x)`: Sigmoid å‡½æ•°ï¼Œè¾“å‡ºèŒƒå›´ (0, 1)
- `relu(x)`: ReLU å‡½æ•°ï¼Œ`max(0, x)`
- `softmax(x)`: Softmax å‡½æ•°ï¼Œç”¨äºå¤šåˆ†ç±»è¾“å‡ºå±‚
- `step_function(x)`: é˜¶è·ƒå‡½æ•°

**æŸå¤±å‡½æ•°**ï¼š
- `mean_squared_error(y, t)`: å‡æ–¹è¯¯å·®ï¼ˆå›å½’ï¼‰
- `cross_entropy_error(y, t)`: äº¤å‰ç†µè¯¯å·®ï¼ˆåˆ†ç±»ï¼‰

**ç¤ºä¾‹**ï¼š
```python
from common.functions import sigmoid, relu, softmax, cross_entropy_error
import numpy as np

x = np.array([-1.0, 1.0, 2.0])
print(sigmoid(x))  # [0.26894142 0.73105858 0.88079708]
print(relu(x))     # [0. 1. 2.]

# è®¡ç®—æŸå¤±
y = np.array([0.1, 0.8, 0.1])  # é¢„æµ‹æ¦‚ç‡
t = np.array([0, 1, 0])        # çœŸå®æ ‡ç­¾
loss = cross_entropy_error(y, t)
```

---

#### `common/layers.py`
ç¥ç»ç½‘ç»œå±‚çš„å®ç°ï¼Œæ”¯æŒå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚

**åŸºç¡€å±‚**ï¼š
- `Relu`: ReLU æ¿€æ´»å±‚
- `Sigmoid`: Sigmoid æ¿€æ´»å±‚
- `Affine`: å…¨è¿æ¥å±‚ï¼ˆçŸ©é˜µä¹˜æ³• + åç½®ï¼‰
- `SoftmaxWithLoss`: Softmax å’Œäº¤å‰ç†µæŸå¤±çš„ç»„åˆ

**CNN å±‚**ï¼š
- `Convolution`: å·ç§¯å±‚
- `Pooling`: æ± åŒ–å±‚

**æ­£åˆ™åŒ–å±‚**ï¼š
- `Dropout`: Dropout å±‚
- `BatchNormalization`: Batch Normalization å±‚

**ç¤ºä¾‹**ï¼š
```python
from common.layers import Affine, Relu, SoftmaxWithLoss
import numpy as np

# æ„å»ºç½‘ç»œ
affine1 = Affine(W1, b1)
relu1 = Relu()
affine2 = Affine(W2, b2)
last_layer = SoftmaxWithLoss()

# å‰å‘ä¼ æ’­
x = np.random.randn(100, 784)  # 100 ä¸ªæ ·æœ¬
out = affine1.forward(x)
out = relu1.forward(out)
out = affine2.forward(out)
loss = last_layer.forward(out, t)

# åå‘ä¼ æ’­
dout = 1
dout = last_layer.backward(dout)
dout = affine2.backward(dout)
dout = relu1.backward(dout)
dout = affine1.backward(dout)
```

---

#### `common/optimizer.py`
å¤šç§ä¼˜åŒ–ç®—æ³•çš„å®ç°ã€‚

**å¯ç”¨ä¼˜åŒ–å™¨**ï¼š
1. **SGD (Stochastic Gradient Descent)**
   ```python
   optimizer = SGD(lr=0.01)
   ```

2. **Momentum**
   ```python
   optimizer = Momentum(lr=0.01, momentum=0.9)
   ```

3. **AdaGrad**
   ```python
   optimizer = AdaGrad(lr=0.01)
   ```

4. **RMSprop**
   ```python
   optimizer = RMSprop(lr=0.01, decay_rate=0.99)
   ```

5. **Adam** (æ¨è)
   ```python
   optimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)
   ```

**ä½¿ç”¨æ–¹æ³•**ï¼š
```python
from common.optimizer import Adam

optimizer = Adam(lr=0.001)

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    # è®¡ç®—æ¢¯åº¦
    grads = network.gradient(x_batch, t_batch)
    
    # æ›´æ–°å‚æ•°
    optimizer.update(network.params, grads)
```

---

#### `common/trainer.py`
å°è£…è®­ç»ƒå¾ªç¯ï¼Œç®€åŒ–è®­ç»ƒä»£ç ã€‚

**ä¸»è¦åŠŸèƒ½**ï¼š
- è‡ªåŠ¨åŒ–è®­ç»ƒå¾ªç¯
- è®°å½•è®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡
- ç»˜åˆ¶å­¦ä¹ æ›²çº¿

**ç¤ºä¾‹**ï¼š
```python
from common.trainer import Trainer
from common.optimizer import Adam

network = YourNetwork()
optimizer = Adam()

trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=20, mini_batch_size=100,
                  optimizer=optimizer, optimizer_param={'lr': 0.001},
                  evaluate_sample_num_per_epoch=1000)
trainer.train()

# ç»˜åˆ¶å­¦ä¹ æ›²çº¿
trainer.plot()
```

---

#### `dataset/mnist.py`
MNIST æ•°æ®é›†åŠ è½½å™¨ï¼Œæ”¯æŒè‡ªåŠ¨ä¸‹è½½å’Œé¢„å¤„ç†ã€‚

**åŠŸèƒ½ç‰¹æ€§**ï¼š
- è‡ªåŠ¨ä»é•œåƒæºä¸‹è½½æ•°æ®é›†ï¼ˆé˜¿é‡Œäº‘é•œåƒï¼‰
- å›¾åƒå½’ä¸€åŒ–ï¼ˆ0-255 â†’ 0.0-1.0ï¼‰
- One-hot ç¼–ç æ ‡ç­¾
- æ”¯æŒå±•å¹³æˆ–ä¿æŒå›¾åƒå½¢çŠ¶

**API è¯´æ˜**ï¼š
```python
from dataset.mnist import load_mnist

# åŠ è½½æ•°æ®é›†
(x_train, t_train), (x_test, t_test) = load_mnist(
    normalize=True,      # å½’ä¸€åŒ–åˆ° [0.0, 1.0]
    flatten=True,        # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ (784,)
    one_hot_label=False  # æ ‡ç­¾ä¸ºæ•´æ•° (0-9)
)

print(x_train.shape)  # (60000, 784)
print(t_train.shape)  # (60000,)
print(x_test.shape)   # (10000, 784)
print(t_test.shape)   # (10000,)
```

**æ•°æ®é›†è§„æ¨¡**ï¼š
- è®­ç»ƒé›†ï¼š60,000 å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š10,000 å¼ å›¾åƒ
- å›¾åƒå°ºå¯¸ï¼š28Ã—28 ç°åº¦å›¾

---

### ğŸ“ å­¦ä¹ è·¯å¾„å»ºè®®

**åˆå­¦è€…è·¯å¾„**ï¼ˆæŒ‰é¡ºåºå­¦ä¹ ï¼‰ï¼š
1. **Chapter 1-2**ï¼šPython åŸºç¡€å’Œæ„ŸçŸ¥æœºï¼ˆ1-2 å¤©ï¼‰
2. **Chapter 3**ï¼šç¥ç»ç½‘ç»œåŸºç¡€å’Œ MNISTï¼ˆ2-3 å¤©ï¼‰
3. **Chapter 4**ï¼šæ¢¯åº¦ä¸‹é™å’Œè®­ç»ƒï¼ˆ3-4 å¤©ï¼‰
4. **Chapter 5**ï¼šåå‘ä¼ æ’­ç®—æ³•ï¼ˆ4-5 å¤©ï¼‰
5. **Chapter 6**ï¼šä¼˜åŒ–æŠ€å·§ï¼ˆ3-4 å¤©ï¼‰
6. **Chapter 7**ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆ4-5 å¤©ï¼‰
7. **Chapter 8**ï¼šæ·±åº¦ç½‘ç»œï¼ˆ3-4 å¤©ï¼‰

**æ€»è®¡å­¦ä¹ æ—¶é—´**ï¼šçº¦ 20-30 å¤©ï¼ˆæ¯å¤© 2-3 å°æ—¶ï¼‰

**è¿›é˜¶è€…è·¯å¾„**ï¼š
- å¦‚æœç†Ÿæ‚‰ Python å’Œ NumPyï¼Œå¯è·³è¿‡ Chapter 1
- å¦‚æœäº†è§£ç¥ç»ç½‘ç»œåŸºç¡€ï¼Œå¯ä» Chapter 4 å¼€å§‹
- é‡ç‚¹å…³æ³¨ Chapter 5ï¼ˆåå‘ä¼ æ’­ï¼‰å’Œ Chapter 6ï¼ˆä¼˜åŒ–æŠ€å·§ï¼‰

**å®è·µå»ºè®®**ï¼š
1. æ¯ç« ä»£ç éƒ½è¦äº²è‡ªè¿è¡Œå’Œè°ƒè¯•
2. å°è¯•ä¿®æ”¹è¶…å‚æ•°è§‚å¯Ÿæ•ˆæœ
3. å¯è§†åŒ–ä¸­é—´ç»“æœï¼ˆæ¿€æ´»å€¼ã€æ¢¯åº¦ã€æŸå¤±æ›²çº¿ï¼‰
4. åœ¨å…¶ä»–æ•°æ®é›†ä¸Šæµ‹è¯•ï¼ˆå¦‚ Fashion-MNISTï¼‰

---

### ğŸ”§ å¸¸è§é—®é¢˜è§£ç­”

#### Q1: MNIST æ•°æ®é›†ä¸‹è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
**A**: é¡¹ç›®å·²é…ç½®é˜¿é‡Œäº‘é•œåƒæºï¼Œé€šå¸¸ä¸ä¼šå¤±è´¥ã€‚å¦‚é‡ç½‘ç»œé—®é¢˜ï¼š
```python
# æ‰‹åŠ¨ä¸‹è½½
cd dataset
python mnist.py
```

#### Q2: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
**A**: 
- å‡å°‘è®­ç»ƒæ ·æœ¬ï¼š`x_train = x_train[:10000]`
- å‡å°‘ epoch æ•°é‡
- ä½¿ç”¨æ›´å¼ºçš„ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰
- å¢å¤§ batch size

#### Q3: å¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šä½¿ç”¨ï¼Ÿ
**A**: 
```python
# å‚è€ƒ dataset/mnist.py å®ç°æ•°æ®åŠ è½½å™¨
# ç¡®ä¿æ•°æ®æ ¼å¼ä¸ºï¼š(N, 784) æˆ– (N, 1, 28, 28)
# æ ‡ç­¾æ ¼å¼ä¸ºï¼š(N,) æˆ– (N, 10)

from common.multi_layer_net import MultiLayerNet

network = MultiLayerNet(input_size=your_input_size, 
                        hidden_size_list=[100, 100],
                        output_size=your_num_classes)
```

#### Q4: å‡ºç° "MemoryError" æ€ä¹ˆåŠï¼Ÿ
**A**: å‡å°‘ batch size æˆ–å‡å°‘éšè—å±‚ç¥ç»å…ƒæ•°é‡ã€‚

#### Q5: å¦‚ä½•ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Ÿ
**A**: 
```python
import pickle

# ä¿å­˜æ¨¡å‹
with open('my_model.pkl', 'wb') as f:
    pickle.dump(network.params, f)

# åŠ è½½æ¨¡å‹
with open('my_model.pkl', 'rb') as f:
    params = pickle.load(f)
    network.params = params
```

#### Q6: å¯ä»¥ç”¨ GPU åŠ é€Ÿå—ï¼Ÿ
**A**: æœ¬é¡¹ç›®ä¸ºæ•™è‚²ç›®çš„ä½¿ç”¨çº¯ NumPy å®ç°ï¼Œä¸æ”¯æŒ GPUã€‚å¦‚éœ€ GPU åŠ é€Ÿï¼Œå»ºè®®ä½¿ç”¨ TensorFlow æˆ– PyTorchã€‚

---

### ğŸ’¡ ä»£ç ç¤ºä¾‹

#### ä»å¤´è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œ
```python
import numpy as np
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.optimizer import Adam

# åŠ è½½æ•°æ®
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# åˆ›å»ºç½‘ç»œ
network = MultiLayerNet(input_size=784, 
                        hidden_size_list=[100, 100],
                        output_size=10)

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = Adam(lr=0.001)

# è®­ç»ƒå‚æ•°
epochs = 20
batch_size = 100
train_size = x_train.shape[0]
iter_per_epoch = max(train_size / batch_size, 1)

for epoch in range(epochs):
    # æ¯ä¸ª epoch
    for i in range(int(iter_per_epoch)):
        # mini-batch
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
        
        # è®¡ç®—æ¢¯åº¦
        grads = network.gradient(x_batch, t_batch)
        
        # æ›´æ–°å‚æ•°
        optimizer.update(network.params, grads)
    
    # è¯„ä¼°
    train_acc = network.accuracy(x_train, t_train)
    test_acc = network.accuracy(x_test, t_test)
    print(f"Epoch {epoch+1}: Train acc = {train_acc:.4f}, Test acc = {test_acc:.4f}")
```

#### ä½¿ç”¨ Trainer ç®€åŒ–è®­ç»ƒ
```python
from common.trainer import Trainer

trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=20, mini_batch_size=100,
                  optimizer='Adam', optimizer_param={'lr': 0.001})
trainer.train()
trainer.plot()  # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
```

---

### ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿å¯¹æœ¬é¡¹ç›®åšå‡ºè´¡çŒ®ï¼

**è´¡çŒ®æ–¹å¼**ï¼š
1. Fork æœ¬ä»“åº“
2. åˆ›å»ºä½ çš„ç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤ä½ çš„ä¿®æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ä¸€ä¸ª Pull Request

**å»ºè®®çš„è´¡çŒ®æ–¹å‘**ï¼š
- ä¿®å¤ bug
- æ”¹è¿›ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£
- æ·»åŠ æ›´å¤šç¤ºä¾‹
- æ€§èƒ½ä¼˜åŒ–
- æ·»åŠ æ–°çš„ä¼˜åŒ–å™¨æˆ–å±‚å®ç°
- æ·»åŠ å•å…ƒæµ‹è¯•

---

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›æ•™è‚²å’Œå­¦ä¹ ä½¿ç”¨ã€‚

---

### ğŸ“š å‚è€ƒèµ„æ–™

**ç›¸å…³ä¹¦ç±**ï¼š
- ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹ï¼ˆåŸä¹¦ï¼‰
- ã€ŠDeep Learningã€‹ by Ian Goodfellow
- ã€ŠNeural Networks and Deep Learningã€‹ by Michael Nielsen

**åœ¨çº¿èµ„æº**ï¼š
- [MNIST Database](http://yann.lecun.com/exdb/mnist/)
- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/)
- [Neural Networks and Deep Learning (å…è´¹åœ¨çº¿ä¹¦ç±)](http://neuralnetworksanddeeplearning.com/)

**æ¨èåç»­å­¦ä¹ **ï¼š
- PyTorch å®˜æ–¹æ•™ç¨‹
- TensorFlow å®˜æ–¹æ•™ç¨‹
- Fast.ai æ·±åº¦å­¦ä¹ è¯¾ç¨‹

---

### ğŸ™ è‡´è°¢

æ„Ÿè°¢ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹ä¸€ä¹¦çš„ä½œè€…ï¼Œä¸ºæ·±åº¦å­¦ä¹ åˆå­¦è€…æä¾›äº†å¦‚æ­¤ä¼˜ç§€çš„å­¦ä¹ ææ–™ã€‚

---

## <a name="english-version"></a>ğŸ“š English Version

### Project Introduction

This repository contains the implementation code for the book *"Deep Learning from Scratch: Python-Based Theory and Implementation"*. It builds neural networks and deep learning algorithms from scratch using pure Python and NumPy, helping readers deeply understand the core principles of deep learning.

**Key Features**:
- ğŸ”§ **Built from Scratch**: All components implemented using NumPy without deep learning frameworks
- ğŸ“– **Step-by-Step Learning**: Organized by chapters, from perceptrons to CNNs
- ğŸ¯ **Hands-On Practice**: Runnable example code and MNIST dataset practice in each chapter
- ğŸ§® **Mathematical Intuition**: Translating abstract mathematical concepts into executable Python code

### Requirements

**Required Dependencies**:
- Python 3.x (3.6+ recommended)
- NumPy >= 1.15.0
- Matplotlib >= 2.0.0
- Pillow (PIL) >= 5.0.0

### Quick Start

#### 1. Clone the Repository

```bash
git clone https://github.com/MengQxuan/fishbook.git
cd fishbook
```

#### 2. Install Dependencies

```bash
pip install numpy matplotlib pillow
```

#### 3. Download MNIST Dataset

The MNIST dataset will be automatically downloaded on first run from a mirror source.

#### 4. Run Examples

```bash
# Test perceptron
python chapter2/and_gate.py

# Run MNIST neural network inference
python chapter3/neuralnet_mnist.py

# Train neural network
python chapter4/train_neuralnet.py
```

### ğŸ“‚ Project Structure

```plaintext
fishbook/
â”œâ”€â”€ chapter1/          # Chapter 1: Python Basics
â”œâ”€â”€ chapter2/          # Chapter 2: Perceptron
â”œâ”€â”€ chapter3/          # Chapter 3: Neural Networks
â”œâ”€â”€ chapter4/          # Chapter 4: Training Neural Networks
â”œâ”€â”€ chapter5/          # Chapter 5: Backpropagation
â”œâ”€â”€ chapter6/          # Chapter 6: Training Techniques
â”œâ”€â”€ chapter7/          # Chapter 7: Convolutional Neural Networks
â”œâ”€â”€ chapter8/          # Chapter 8: Deep Learning
â”œâ”€â”€ common/            # Common Modules
â”‚   â”œâ”€â”€ functions.py   # Activation and loss functions
â”‚   â”œâ”€â”€ layers.py      # Layer implementations
â”‚   â”œâ”€â”€ optimizer.py   # Optimizers (SGD, Adam, etc.)
â”‚   â”œâ”€â”€ trainer.py     # Training utilities
â”‚   â””â”€â”€ util.py        # Helper functions
â””â”€â”€ dataset/           # Dataset Module
    â”œâ”€â”€ mnist.py       # MNIST data loader
    â””â”€â”€ mnist.pkl      # MNIST dataset (auto-downloaded)
```

### ğŸ“– Chapter Overview

- **Chapter 1**: Python basics including NumPy arrays and broadcasting
- **Chapter 2**: Perceptron and logic gates (AND, OR, NAND, XOR)
- **Chapter 3**: Neural network fundamentals, activation functions, MNIST inference
- **Chapter 4**: Training neural networks with gradient descent
- **Chapter 5**: Backpropagation algorithm and computational graphs
- **Chapter 6**: Optimization techniques (Adam, Dropout, Batch Normalization)
- **Chapter 7**: Convolutional Neural Networks (CNNs)
- **Chapter 8**: Deep learning and advanced networks (99%+ accuracy on MNIST)

### ğŸ§° Common Modules

#### `common/functions.py`
Core mathematical functions including:
- Activation functions: `sigmoid()`, `relu()`, `softmax()`
- Loss functions: `mean_squared_error()`, `cross_entropy_error()`

#### `common/layers.py`
Neural network layer implementations:
- Basic layers: `Relu`, `Sigmoid`, `Affine`, `SoftmaxWithLoss`
- CNN layers: `Convolution`, `Pooling`
- Regularization: `Dropout`, `BatchNormalization`

#### `common/optimizer.py`
Optimization algorithms:
- `SGD`: Stochastic Gradient Descent
- `Momentum`: SGD with momentum
- `AdaGrad`: Adaptive learning rates
- `RMSprop`: Root Mean Square Propagation
- `Adam`: Adaptive Moment Estimation (recommended)

#### `dataset/mnist.py`
MNIST dataset loader with automatic download and preprocessing.

### ğŸ’¡ Code Example

```python
import numpy as np
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.optimizer import Adam

# Load data
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# Create network
network = MultiLayerNet(input_size=784, 
                        hidden_size_list=[100, 100],
                        output_size=10)

# Create optimizer
optimizer = Adam(lr=0.001)

# Training loop
epochs = 20
batch_size = 100

for epoch in range(epochs):
    # Mini-batch training
    batch_mask = np.random.choice(len(x_train), batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # Calculate gradients
    grads = network.gradient(x_batch, t_batch)
    
    # Update parameters
    optimizer.update(network.params, grads)
    
    # Evaluate
    train_acc = network.accuracy(x_train, t_train)
    test_acc = network.accuracy(x_test, t_test)
    print(f"Epoch {epoch+1}: Train={train_acc:.4f}, Test={test_acc:.4f}")
```

### ğŸ“ Learning Path

**Beginner Path** (follow in order):
1. Chapters 1-2: Python basics and perceptron (1-2 days)
2. Chapter 3: Neural network basics (2-3 days)
3. Chapter 4: Training with gradient descent (3-4 days)
4. Chapter 5: Backpropagation (4-5 days)
5. Chapter 6: Optimization techniques (3-4 days)
6. Chapter 7: CNNs (4-5 days)
7. Chapter 8: Deep networks (3-4 days)

**Total Learning Time**: ~20-30 days (2-3 hours per day)

### ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### ğŸ“„ License

This project is for educational purposes only.

### ğŸ“š References

- *Deep Learning from Scratch* (Original Book)
- [MNIST Database](http://yann.lecun.com/exdb/mnist/)
- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/)

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Starï¼**

**â­ Star this project if you find it helpful!**

</div>