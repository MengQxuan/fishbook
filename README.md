# Fishbook - ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹

<div align="center">

[![Python Version](https://img.shields.io/badge/python-3.x-blue.svg)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/numpy-required-green.svg)](https://numpy.org/)
[![License](https://img.shields.io/badge/license-Educational-orange.svg)](LICENSE)

*ä»é›¶å¼€å§‹å­¦ä¹ æ·±åº¦å­¦ä¹  - ä½¿ç”¨ Python å’Œ NumPy æ„å»ºç¥ç»ç½‘ç»œ*

[English](#english-version) | [ä¸­æ–‡](#chinese-version)

</div>

---
### é¡¹ç›®ç®€ä»‹

æœ¬ä»“åº“æ˜¯ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹ä¸€ä¹¦çš„é…å¥—ä»£ç å®ç°ã€‚é€šè¿‡çº¯ Python å’Œ NumPyï¼Œä»é›¶å¼€å§‹æ„å»ºç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œå¸®åŠ©è¯»è€…æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒåŸç†ã€‚

**æ ¸å¿ƒç‰¹ç‚¹**ï¼š
- ğŸ”§ **ä»é›¶å®ç°**ï¼šä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨ NumPy æ‰‹å·¥å®ç°æ‰€æœ‰ç»„ä»¶
- ğŸ“– **å¾ªåºæ¸è¿›**ï¼šæŒ‰ç« èŠ‚ç»„ç»‡ï¼Œä»æ„ŸçŸ¥æœºåˆ°å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€æ­¥æ·±å…¥
- ğŸ¯ **å®æˆ˜å¯¼å‘**ï¼šæ¯ç« åŒ…å«å¯è¿è¡Œçš„ç¤ºä¾‹ä»£ç å’Œ MNIST æ•°æ®é›†å®è·µ
- ğŸ§® **æ•°å­¦ç›´è§‚**ï¼šå°†æŠ½è±¡çš„æ•°å­¦æ¦‚å¿µè½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„ Python ä»£ç 

### ç¯å¢ƒè¦æ±‚

**å¿…éœ€ä¾èµ–**ï¼š
- Python 3.x (æ¨è 3.6 æˆ–æ›´é«˜ç‰ˆæœ¬)
- NumPy >= 1.15.0
- Matplotlib >= 2.0.0
- Pillow (PIL) >= 5.0.0

### å¿«é€Ÿå¼€å§‹

#### 1. å…‹éš†ä»“åº“

```bash
git clone https://github.com/MengQxuan/fishbook.git
cd fishbook
```

#### 2. é±¼ä¹¦é…å¥—æºä»£ç  + MNIST æ•°æ®é›†

```bash
é€šè¿‡ç½‘ç›˜åˆ†äº«çš„æ–‡ä»¶ï¼šMNISTç­‰2ä¸ªæ–‡ä»¶
é“¾æ¥: https://pan.baidu.com/s/1THFM60LqHEJtftWiJuZmwA?pwd=n3t5 æå–ç : n3t5 
--æ¥è‡ªç™¾åº¦ç½‘ç›˜è¶…çº§ä¼šå‘˜v3çš„åˆ†äº«
```


### ğŸ“‚ é¡¹ç›®ç»“æ„è¯¦è§£

```plaintext
fishbook/
â”œâ”€â”€ chapter1/                   # ç¬¬1ç« ï¼šPython å…¥é—¨
â”‚   â””â”€â”€ hello.py               # Python åŸºç¡€ï¼šå˜é‡ã€ç±»å‹ã€ç±»ã€NumPy å¹¿æ’­
â”‚
â”œâ”€â”€ chapter2/                   # ç¬¬2ç« ï¼šæ„ŸçŸ¥æœº
â”‚   â”œâ”€â”€ and_gate.py            # AND é€»è¾‘é—¨å®ç°
â”‚   â”œâ”€â”€ or_gate.py             # OR é€»è¾‘é—¨å®ç°
â”‚   â”œâ”€â”€ nand_gate.py           # NAND é€»è¾‘é—¨å®ç°
â”‚   â””â”€â”€ xor_gate.py            # XOR é€»è¾‘é—¨ï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰
â”‚
â”œâ”€â”€ chapter3/                   # ç¬¬3ç« ï¼šç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ sigmoid.py             # Sigmoid æ¿€æ´»å‡½æ•°åŠå¯è§†åŒ–
â”‚   â”œâ”€â”€ relu.py                # ReLU æ¿€æ´»å‡½æ•°
â”‚   â”œâ”€â”€ step_function.py       # é˜¶è·ƒå‡½æ•°
â”‚   â”œâ”€â”€ forward.py             # å‰å‘ä¼ æ’­ç¤ºä¾‹
â”‚   â”œâ”€â”€ mnist_show.py          # æ˜¾ç¤º MNIST å›¾åƒ
â”‚   â”œâ”€â”€ neuralnet_mnist.py     # ç¥ç»ç½‘ç»œæ¨ç†ï¼ˆå•æ ·æœ¬ï¼‰
â”‚   â”œâ”€â”€ neuralnet_mnist_batch.py  # æ‰¹é‡æ¨ç†ï¼ˆæå‡æ€§èƒ½ï¼‰
â”‚   â””â”€â”€ sample_weight.pkl      # é¢„è®­ç»ƒæƒé‡æ–‡ä»¶
â”‚
â”œâ”€â”€ chapter4/                   # ç¬¬4ç« ï¼šç¥ç»ç½‘ç»œçš„å­¦ä¹ 
â”‚   â”œâ”€â”€ gradient_1d.py         # ä¸€ç»´å‡½æ•°çš„æ¢¯åº¦ï¼ˆæ•°å€¼å¾®åˆ†ï¼‰
â”‚   â”œâ”€â”€ gradient_2d.py         # äºŒç»´å‡½æ•°çš„æ¢¯åº¦å¯è§†åŒ–
â”‚   â”œâ”€â”€ gradient_method.py     # æ¢¯åº¦ä¸‹é™æ³•
â”‚   â”œâ”€â”€ gradient_simplenet.py  # ç®€å•ç½‘ç»œçš„æ¢¯åº¦è®¡ç®—
â”‚   â”œâ”€â”€ two_layer_net.py       # ä¸¤å±‚ç¥ç»ç½‘ç»œç±»
â”‚   â””â”€â”€ train_neuralnet.py     # å®Œæ•´è®­ç»ƒæµç¨‹
â”‚
â”œâ”€â”€ chapter5/                   # ç¬¬5ç« ï¼šè¯¯å·®åå‘ä¼ æ’­æ³•
â”‚   â”œâ”€â”€ buy_apple.py           # è®¡ç®—å›¾ç¤ºä¾‹ï¼ˆç®€å•ï¼‰
â”‚   â”œâ”€â”€ buy_apple_orange.py    # è®¡ç®—å›¾ç¤ºä¾‹ï¼ˆå¤æ‚ï¼‰
â”‚   â”œâ”€â”€ layer_naive.py         # ä¹˜æ³•å±‚å’ŒåŠ æ³•å±‚çš„ç®€å•å®ç°
â”‚   â”œâ”€â”€ gradient_check.py      # æ¢¯åº¦æ£€éªŒï¼ˆå¯¹æ¯”æ•°å€¼æ¢¯åº¦å’Œåå‘ä¼ æ’­ï¼‰
â”‚   â”œâ”€â”€ two_layer_net.py       # ä½¿ç”¨å±‚çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ
â”‚   â””â”€â”€ train_neuralnet.py     # ä½¿ç”¨åå‘ä¼ æ’­çš„è®­ç»ƒ
â”‚
â”œâ”€â”€ chapter6/                   # ç¬¬6ç« ï¼šä¸å­¦ä¹ ç›¸å…³çš„æŠ€å·§
â”‚   â”œâ”€â”€ optimizer_compare_naive.py    # ä¼˜åŒ–å™¨æ¯”è¾ƒï¼ˆç®€å•å‡½æ•°ï¼‰
â”‚   â”œâ”€â”€ optimizer_compare_mnist.py    # ä¼˜åŒ–å™¨åœ¨ MNIST ä¸Šçš„æ¯”è¾ƒ
â”‚   â”œâ”€â”€ weight_init_activation_histogram.py  # æƒé‡åˆå§‹åŒ–å¯¹æ¿€æ´»å€¼åˆ†å¸ƒçš„å½±å“
â”‚   â”œâ”€â”€ weight_init_compare.py        # æƒé‡åˆå§‹åŒ–æ–¹æ³•æ¯”è¾ƒ
â”‚   â”œâ”€â”€ batch_norm_test.py            # Batch Normalization æ•ˆæœæµ‹è¯•
â”‚   â”œâ”€â”€ overfit_weight_decay.py       # æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰
â”‚   â”œâ”€â”€ overfit_dropout.py            # Dropout æ­£åˆ™åŒ–
â”‚   â””â”€â”€ hyperparameter_optimization.py  # è¶…å‚æ•°ä¼˜åŒ–
â”‚
â”œâ”€â”€ chapter7/                   # ç¬¬7ç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ simple_convnet.py      # ç®€å•å·ç§¯ç¥ç»ç½‘ç»œå®ç°
â”‚   â”œâ”€â”€ train_convnet.py       # è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ visualize_filter.py    # å¯è§†åŒ–å·ç§¯æ»¤æ³¢å™¨
â”‚   â””â”€â”€ params.pkl             # é¢„è®­ç»ƒçš„å·ç§¯ç½‘ç»œæƒé‡
â”‚
â”œâ”€â”€ chapter8/                   # ç¬¬8ç« ï¼šæ·±åº¦å­¦ä¹ 
â”‚   â”œâ”€â”€ deep_convnet.py        # æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ99%+ å‡†ç¡®ç‡ï¼‰
â”‚   â”œâ”€â”€ train_deepnet.py       # è®­ç»ƒæ·±åº¦ç½‘ç»œ
â”‚   â””â”€â”€ deep_convnet_params.pkl  # æ·±åº¦ç½‘ç»œé¢„è®­ç»ƒæƒé‡
â”‚
â”œâ”€â”€ common/                     # å…¬å…±æ¨¡å—
â”‚   â”œâ”€â”€ functions.py           # æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ gradient.py            # æ•°å€¼æ¢¯åº¦è®¡ç®—
â”‚   â”œâ”€â”€ layers.py              # å±‚çš„å®ç°ï¼ˆAffineã€Softmaxã€Convã€Poolingï¼‰
â”‚   â”œâ”€â”€ multi_layer_net.py     # å¤šå±‚ç¥ç»ç½‘ç»œ
â”‚   â”œâ”€â”€ multi_layer_net_extend.py  # æ‰©å±•çš„å¤šå±‚ç½‘ç»œï¼ˆæ”¯æŒ Dropoutã€BNï¼‰
â”‚   â”œâ”€â”€ optimizer.py           # ä¼˜åŒ–å™¨ï¼ˆSGDã€Momentumã€AdaGradã€Adamï¼‰
â”‚   â”œâ”€â”€ trainer.py             # è®­ç»ƒå™¨ï¼ˆå°è£…è®­ç»ƒå¾ªç¯ï¼‰
â”‚   â””â”€â”€ util.py                # è¾…åŠ©å‡½æ•°
â”‚
â””â”€â”€ dataset/                    # æ•°æ®é›†æ¨¡å—
    â”œâ”€â”€ mnist.py               # MNIST æ•°æ®åŠ è½½å™¨
    â””â”€â”€ mnist.pkl              # MNIST æ•°æ®é›†
```

### ğŸ“– ç« èŠ‚è¯¦ç»†è¯´æ˜

#### Chapter 1: Python å…¥é—¨
ä»‹ç» Python ç¼–ç¨‹åŸºç¡€ï¼ŒåŒ…æ‹¬å˜é‡ã€æ•°æ®ç±»å‹ã€ç±»ã€NumPy çš„åŸºæœ¬ä½¿ç”¨å’Œå¹¿æ’­æœºåˆ¶ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- Python åŸºæœ¬è¯­æ³•
- ç±»ä¸å¯¹è±¡
- NumPy æ•°ç»„æ“ä½œå’Œå¹¿æ’­


---

#### Chapter 2: æ„ŸçŸ¥æœº
å®ç°æœ€ç®€å•çš„ç¥ç»ç½‘ç»œâ€”â€”æ„ŸçŸ¥æœºï¼Œå¹¶é€šè¿‡ç»„åˆå®ç°é€»è¾‘é—¨ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æ„ŸçŸ¥æœºçš„åŸºæœ¬åŸç†
- å®ç° ANDã€ORã€NAND é—¨
- å¤šå±‚æ„ŸçŸ¥æœºå®ç° XOR é—¨

**å­¦ä¹ è¦ç‚¹**ï¼š
- ç†è§£çº¿æ€§åˆ†ç±»å™¨çš„å±€é™æ€§
- äº†è§£å¤šå±‚æ„ŸçŸ¥æœºçš„è¡¨è¾¾èƒ½åŠ›

---

#### Chapter 3: ç¥ç»ç½‘ç»œ
æ·±å…¥ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ¦‚å¿µï¼ŒåŒ…æ‹¬æ¿€æ´»å‡½æ•°ã€å‰å‘ä¼ æ’­å’Œ MNIST æ•°æ®é›†çš„æ¨ç†ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æ¿€æ´»å‡½æ•°ï¼ˆSigmoidã€ReLUã€Step Functionï¼‰
- ä¸‰å±‚ç¥ç»ç½‘ç»œçš„å®ç°
- MNIST æ‰‹å†™æ•°å­—è¯†åˆ«
- æ‰¹å¤„ç†ä¼˜åŒ–

**å…³é”®æ–‡ä»¶**ï¼š
- `sigmoid.py` / `relu.py`: æ¿€æ´»å‡½æ•°çš„å®ç°å’Œå¯è§†åŒ–
- `neuralnet_mnist.py`: ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œæ¨ç†
- `neuralnet_mnist_batch.py`: æ‰¹é‡å¤„ç†æå‡æ¨ç†é€Ÿåº¦

---

#### Chapter 4: ç¥ç»ç½‘ç»œçš„å­¦ä¹ 
å­¦ä¹ å¦‚ä½•é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è®­ç»ƒç¥ç»ç½‘ç»œã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æŸå¤±å‡½æ•°ï¼ˆå‡æ–¹è¯¯å·®ã€äº¤å‰ç†µï¼‰
- æ•°å€¼å¾®åˆ†å’Œæ¢¯åº¦è®¡ç®—
- æ¢¯åº¦ä¸‹é™æ³•
- mini-batch å­¦ä¹ 

**å…³é”®æ–‡ä»¶**ï¼š
- `gradient_1d.py` / `gradient_2d.py`: æ¢¯åº¦å¯è§†åŒ–
- `gradient_method.py`: æ¢¯åº¦ä¸‹é™æ³•æ¼”ç¤º
- `two_layer_net.py`: ä¸¤å±‚ç¥ç»ç½‘ç»œç±»
- `train_neuralnet.py`: å®Œæ•´çš„è®­ç»ƒæµç¨‹


**è®­ç»ƒè¾“å‡º**ï¼š
- é€ä¸ª epoch æ˜¾ç¤ºè®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡
- æŸå¤±å‡½æ•°å€¼çš„å˜åŒ–æ›²çº¿

---

#### Chapter 5: è¯¯å·®åå‘ä¼ æ’­æ³•
ä½¿ç”¨è®¡ç®—å›¾å’Œé“¾å¼æ³•åˆ™å®ç°é«˜æ•ˆçš„åå‘ä¼ æ’­ç®—æ³•ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- è®¡ç®—å›¾çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
- å„å±‚çš„åå‘ä¼ æ’­å®ç°ï¼ˆAffineã€ReLUã€Softmaxï¼‰
- æ¢¯åº¦æ£€éªŒ
- ä½¿ç”¨å±‚æ„å»ºç¥ç»ç½‘ç»œ

**å…³é”®æ–‡ä»¶**ï¼š
- `buy_apple.py`: è®¡ç®—å›¾åŸºç¡€ç¤ºä¾‹
- `layer_naive.py`: å±‚çš„ç®€å•å®ç°
- `gradient_check.py`: éªŒè¯åå‘ä¼ æ’­çš„æ­£ç¡®æ€§
- `train_neuralnet.py`: ä½¿ç”¨åå‘ä¼ æ’­è®­ç»ƒ


**å­¦ä¹ è¦ç‚¹**ï¼š
- ç†è§£è®¡ç®—å›¾å’Œé“¾å¼æ³•åˆ™
- æŒæ¡åå‘ä¼ æ’­çš„é«˜æ•ˆæ€§ï¼ˆæ¯”æ•°å€¼å¾®åˆ†å¿« 100+ å€ï¼‰

---

#### Chapter 6: ä¸å­¦ä¹ ç›¸å…³çš„æŠ€å·§
æ·±åº¦å­¦ä¹ ä¸­çš„é«˜çº§æŠ€å·§ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ã€æƒé‡åˆå§‹åŒ–ã€æ­£åˆ™åŒ–å’Œè¶…å‚æ•°è°ƒä¼˜ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- **ä¼˜åŒ–å™¨**ï¼šSGDã€Momentumã€AdaGradã€RMSpropã€Adam
- **æƒé‡åˆå§‹åŒ–**ï¼šXavier åˆå§‹åŒ–ã€He åˆå§‹åŒ–
- **Batch Normalization**ï¼šåŠ é€Ÿè®­ç»ƒã€æé«˜æ³›åŒ–
- **æ­£åˆ™åŒ–**ï¼šWeight Decayã€Dropout
- **è¶…å‚æ•°ä¼˜åŒ–**ï¼šéªŒè¯é›†ã€ç½‘æ ¼æœç´¢

**å…³é”®æ–‡ä»¶**ï¼š
- `optimizer_compare_mnist.py`: å¯¹æ¯”å„ä¼˜åŒ–å™¨åœ¨ MNIST ä¸Šçš„è¡¨ç°
- `weight_init_compare.py`: æƒé‡åˆå§‹åŒ–æ–¹æ³•å¯¹æ¯”
- `batch_norm_test.py`: BN çš„æ•ˆæœ
- `overfit_dropout.py`: Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
- `hyperparameter_optimization.py`: è¶…å‚æ•°æœç´¢


**å­¦ä¹ è¦ç‚¹**ï¼š
- Adam é€šå¸¸æ˜¯æœ€ä½³é€‰æ‹©
- åˆç†çš„æƒé‡åˆå§‹åŒ–è‡³å…³é‡è¦
- Dropout å’Œ BN å¯æœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆ

---

#### Chapter 7: å·ç§¯ç¥ç»ç½‘ç»œ (CNN)
å®ç°ç”¨äºå›¾åƒè¯†åˆ«çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚

**ä¸»è¦å†…å®¹**ï¼š
- å·ç§¯å±‚ï¼ˆConvolutionï¼‰çš„å®ç°
- æ± åŒ–å±‚ï¼ˆPoolingï¼‰çš„å®ç°
- CNN çš„æ•´ä½“ç»“æ„
- å·ç§¯æ»¤æ³¢å™¨çš„å¯è§†åŒ–

**ç½‘ç»œç»“æ„**ï¼š
```
Input â†’ Conv â†’ ReLU â†’ Pooling â†’ Affine â†’ ReLU â†’ Affine â†’ Softmax
```

**å…³é”®æ–‡ä»¶**ï¼š
- `simple_convnet.py`: ç®€å• CNN å®ç°
- `train_convnet.py`: è®­ç»ƒ CNNï¼ˆå‡†ç¡®ç‡ ~98%ï¼‰
- `visualize_filter.py`: å¯è§†åŒ–å­¦åˆ°çš„æ»¤æ³¢å™¨


---

#### Chapter 8: æ·±åº¦å­¦ä¹ 
æ„å»ºæ›´æ·±çš„ç½‘ç»œï¼Œæ¢ç´¢æ·±åº¦å­¦ä¹ çš„å¨åŠ›ã€‚

**ä¸»è¦å†…å®¹**ï¼š
- æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ
- ç½‘ç»œåŠ æ·±çš„æŠ€å·§
- æ•°æ®å¢å¼º
- è¿ç§»å­¦ä¹ çš„åŸºç¡€

**ç½‘ç»œç»“æ„**ï¼š
```
Conv-ReLU-Conv-ReLU-Pool Ã—3 â†’ Affine-ReLU-Dropout-Affine-Dropout-Softmax
```

**å…³é”®æ–‡ä»¶**ï¼š
- `deep_convnet.py`: æ·±åº¦ CNNï¼ˆ99%+ å‡†ç¡®ç‡ï¼‰
- `train_deepnet.py`: è®­ç»ƒæ·±åº¦ç½‘ç»œ


---

### ğŸ§° å…¬å…±æ¨¡å—è¯¦è§£

#### `common/functions.py`
æ ¸å¿ƒæ•°å­¦å‡½æ•°å®ç°ã€‚

**æ¿€æ´»å‡½æ•°**ï¼š
- `sigmoid(x)`: Sigmoid å‡½æ•°ï¼Œè¾“å‡ºèŒƒå›´ (0, 1)
- `relu(x)`: ReLU å‡½æ•°ï¼Œ`max(0, x)`
- `softmax(x)`: Softmax å‡½æ•°ï¼Œç”¨äºå¤šåˆ†ç±»è¾“å‡ºå±‚
- `step_function(x)`: é˜¶è·ƒå‡½æ•°

**æŸå¤±å‡½æ•°**ï¼š
- `mean_squared_error(y, t)`: å‡æ–¹è¯¯å·®ï¼ˆå›å½’ï¼‰
- `cross_entropy_error(y, t)`: äº¤å‰ç†µè¯¯å·®ï¼ˆåˆ†ç±»ï¼‰

---

#### `common/layers.py`
ç¥ç»ç½‘ç»œå±‚çš„å®ç°ï¼Œæ”¯æŒå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚

**åŸºç¡€å±‚**ï¼š
- `Relu`: ReLU æ¿€æ´»å±‚
- `Sigmoid`: Sigmoid æ¿€æ´»å±‚
- `Affine`: å…¨è¿æ¥å±‚ï¼ˆçŸ©é˜µä¹˜æ³• + åç½®ï¼‰
- `SoftmaxWithLoss`: Softmax å’Œäº¤å‰ç†µæŸå¤±çš„ç»„åˆ

**CNN å±‚**ï¼š
- `Convolution`: å·ç§¯å±‚
- `Pooling`: æ± åŒ–å±‚

**æ­£åˆ™åŒ–å±‚**ï¼š
- `Dropout`: Dropout å±‚
- `BatchNormalization`: Batch Normalization å±‚

---

#### `common/optimizer.py`
å¤šç§ä¼˜åŒ–ç®—æ³•çš„å®ç°ã€‚

**å¯ç”¨ä¼˜åŒ–å™¨**ï¼š
1. **SGD (Stochastic Gradient Descent)**
   ```python
   optimizer = SGD(lr=0.01)
   ```

2. **Momentum**
   ```python
   optimizer = Momentum(lr=0.01, momentum=0.9)
   ```

3. **AdaGrad**
   ```python
   optimizer = AdaGrad(lr=0.01)
   ```

4. **RMSprop**
   ```python
   optimizer = RMSprop(lr=0.01, decay_rate=0.99)
   ```

5. **Adam** (æ¨è)
   ```python
   optimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)
   ```

**ä½¿ç”¨æ–¹æ³•**ï¼š
```python
from common.optimizer import Adam

optimizer = Adam(lr=0.001)

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    # è®¡ç®—æ¢¯åº¦
    grads = network.gradient(x_batch, t_batch)
    
    # æ›´æ–°å‚æ•°
    optimizer.update(network.params, grads)
```

---

#### `common/trainer.py`
å°è£…è®­ç»ƒå¾ªç¯ï¼Œç®€åŒ–è®­ç»ƒä»£ç ã€‚

**ä¸»è¦åŠŸèƒ½**ï¼š
- è‡ªåŠ¨åŒ–è®­ç»ƒå¾ªç¯
- è®°å½•è®­ç»ƒ/æµ‹è¯•å‡†ç¡®ç‡
- ç»˜åˆ¶å­¦ä¹ æ›²çº¿

**ç¤ºä¾‹**ï¼š
```python
from common.trainer import Trainer
from common.optimizer import Adam

network = YourNetwork()
optimizer = Adam()

trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=20, mini_batch_size=100,
                  optimizer=optimizer, optimizer_param={'lr': 0.001},
                  evaluate_sample_num_per_epoch=1000)
trainer.train()

# ç»˜åˆ¶å­¦ä¹ æ›²çº¿
trainer.plot()
```

---

#### `dataset/mnist.py`
MNIST æ•°æ®é›†åŠ è½½å™¨ï¼Œæ”¯æŒè‡ªåŠ¨ä¸‹è½½å’Œé¢„å¤„ç†ã€‚

**åŠŸèƒ½ç‰¹æ€§**ï¼š
- è‡ªåŠ¨ä»é•œåƒæºä¸‹è½½æ•°æ®é›†ï¼ˆé˜¿é‡Œäº‘é•œåƒï¼‰
- å›¾åƒå½’ä¸€åŒ–ï¼ˆ0-255 â†’ 0.0-1.0ï¼‰
- One-hot ç¼–ç æ ‡ç­¾
- æ”¯æŒå±•å¹³æˆ–ä¿æŒå›¾åƒå½¢çŠ¶

**API è¯´æ˜**ï¼š
```python
from dataset.mnist import load_mnist

# åŠ è½½æ•°æ®é›†
(x_train, t_train), (x_test, t_test) = load_mnist(
    normalize=True,      # å½’ä¸€åŒ–åˆ° [0.0, 1.0]
    flatten=True,        # å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ (784,)
    one_hot_label=False  # æ ‡ç­¾ä¸ºæ•´æ•° (0-9)
)

print(x_train.shape)  # (60000, 784)
print(t_train.shape)  # (60000,)
print(x_test.shape)   # (10000, 784)
print(t_test.shape)   # (10000,)
```

**æ•°æ®é›†è§„æ¨¡**ï¼š
- è®­ç»ƒé›†ï¼š60,000 å¼ å›¾åƒ
- æµ‹è¯•é›†ï¼š10,000 å¼ å›¾åƒ
- å›¾åƒå°ºå¯¸ï¼š28Ã—28 ç°åº¦å›¾

---
### ğŸ’¡ ä»£ç ç¤ºä¾‹

#### ä»å¤´è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œ
```python
import numpy as np
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.optimizer import Adam

# åŠ è½½æ•°æ®
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# åˆ›å»ºç½‘ç»œ
network = MultiLayerNet(input_size=784, 
                        hidden_size_list=[100, 100],
                        output_size=10)

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = Adam(lr=0.001)

# è®­ç»ƒå‚æ•°
epochs = 20
batch_size = 100
train_size = x_train.shape[0]
iter_per_epoch = max(train_size / batch_size, 1)

for epoch in range(epochs):
    # æ¯ä¸ª epoch
    for i in range(int(iter_per_epoch)):
        # mini-batch
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
        
        # è®¡ç®—æ¢¯åº¦
        grads = network.gradient(x_batch, t_batch)
        
        # æ›´æ–°å‚æ•°
        optimizer.update(network.params, grads)
    
    # è¯„ä¼°
    train_acc = network.accuracy(x_train, t_train)
    test_acc = network.accuracy(x_test, t_test)
    print(f"Epoch {epoch+1}: Train acc = {train_acc:.4f}, Test acc = {test_acc:.4f}")
```

#### ä½¿ç”¨ Trainer ç®€åŒ–è®­ç»ƒ
```python
from common.trainer import Trainer

trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=20, mini_batch_size=100,
                  optimizer='Adam', optimizer_param={'lr': 0.001})
trainer.train()
trainer.plot()  # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
```

---


### ğŸ“š å‚è€ƒèµ„æ–™

**ç›¸å…³ä¹¦ç±**ï¼š
- ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹ï¼ˆåŸä¹¦ï¼‰
- ã€ŠDeep Learningã€‹ by Ian Goodfellow
- ã€ŠNeural Networks and Deep Learningã€‹ by Michael Nielsen

**åœ¨çº¿èµ„æº**ï¼š
- [MNIST Database](http://yann.lecun.com/exdb/mnist/)
- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/)
- [Neural Networks and Deep Learning (å…è´¹åœ¨çº¿ä¹¦ç±)](http://neuralnetworksanddeeplearning.com/)

**æ¨èåç»­å­¦ä¹ **ï¼š
- PyTorch å®˜æ–¹æ•™ç¨‹
- TensorFlow å®˜æ–¹æ•™ç¨‹
- Fast.ai æ·±åº¦å­¦ä¹ è¯¾ç¨‹

---

### ğŸ™ è‡´è°¢

æ„Ÿè°¢ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹ä¸€ä¹¦çš„ä½œè€…ï¼Œä¸ºæ·±åº¦å­¦ä¹ åˆå­¦è€…æä¾›äº†å¦‚æ­¤ä¼˜ç§€çš„å­¦ä¹ ææ–™ã€‚

---
<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Starï¼**

**â­ Star this project if you find it helpful!**

</div>